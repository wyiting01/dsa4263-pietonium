{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.11.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.11.0)\n",
      "Requirement already satisfied: transformers==4.27.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (4.27.1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (4.65.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.5.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.24.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (3.7.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.2.2)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/claudia/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: packaging in /Users/claudia/Library/Python/3.9/lib/python/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (2.11.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (3.19.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (2.11.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (23.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (0.31.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (2.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (15.0.6.1)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (58.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (1.51.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (3.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (3.10.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/claudia/Library/Python/3.9/lib/python/site-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2022.7.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (4.39.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (9.4.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 6)) (5.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.11.0->-r requirements.txt (line 1)) (0.40.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/claudia/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 6)) (3.15.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (2.16.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (2022.12.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/claudia/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->-r requirements.txt (line 1)) (3.2.2)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n"
     ]
    }
   ],
   "source": [
    "# Install requirements\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>1</td>\n",
       "      <td>6/7/21</td>\n",
       "      <td>Well, I am neither British, nor British-born, ...</td>\n",
       "      <td>well neither british british bear absolutely l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4655</th>\n",
       "      <td>0</td>\n",
       "      <td>4/8/21</td>\n",
       "      <td>I did not know anything about chutney until I ...</td>\n",
       "      <td>know anything chutney know recipe cite find mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>1</td>\n",
       "      <td>1/3/20</td>\n",
       "      <td>I wasn't sure that we would be able to find a ...</td>\n",
       "      <td>sure would able find powdered milk family woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1</td>\n",
       "      <td>11/9/21</td>\n",
       "      <td>We have a 4 year old granddaughter and another...</td>\n",
       "      <td>year old granddaughter another grand baby way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>1</td>\n",
       "      <td>22/5/21</td>\n",
       "      <td>My daughter loves the food, and I like it even...</td>\n",
       "      <td>daughter love food like even better organic am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment     Time                                               Text  \\\n",
       "2262          1   6/7/21  Well, I am neither British, nor British-born, ...   \n",
       "4655          0   4/8/21  I did not know anything about chutney until I ...   \n",
       "1679          1   1/3/20  I wasn't sure that we would be able to find a ...   \n",
       "274           1  11/9/21  We have a 4 year old granddaughter and another...   \n",
       "4092          1  22/5/21  My daughter loves the food, and I like it even...   \n",
       "\n",
       "                                         processed_text  \n",
       "2262  well neither british british bear absolutely l...  \n",
       "4655  know anything chutney know recipe cite find mi...  \n",
       "1679  sure would able find powdered milk family woul...  \n",
       "274   year old granddaughter another grand baby way ...  \n",
       "4092  daughter love food like even better organic am...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read cleaned data\n",
    "df = pd.read_csv(\"../../data/curated/reviews/yiting_cleaned_reviews.csv\")\n",
    "\n",
    "# Display 5 random samples\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>healthy dog food good digestion also good smal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pleased natural balance dog food dog issue dog...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>educate feline nutrition allow cat become addi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>holistic vet recommend along brand try cat pre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buy coffee much cheaper ganocafe organic reish...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  healthy dog food good digestion also good smal...       1\n",
       "1  pleased natural balance dog food dog issue dog...       1\n",
       "2  educate feline nutrition allow cat become addi...       1\n",
       "3  holistic vet recommend along brand try cat pre...       1\n",
       "4  buy coffee much cheaper ganocafe organic reish...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-labelling of columns headers\n",
    "df.rename(columns = {'Sentiment' : 'labels', 'processed_text' : 'text'}, inplace = True)\n",
    "\n",
    "# Extracting out the necessary columns\n",
    "df = df[['text','labels']]\n",
    "\n",
    "# Display the current dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5444"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels values to get the size\n",
    "arr = df['labels'].values\n",
    "arr.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Shape: (5444, 2)\n",
      "Label Shape: (5444, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating 2D array to indicate which row of data the label belongs to\n",
    "labels = np.zeros((arr.size, arr.max() + 1), dtype=int)\n",
    "print(f\"Label Shape: {labels.shape}\")\n",
    "\n",
    "# Indicating the label (0 or 1) of the respective row of data\n",
    "## [1, 0] indicates negative sentiment\n",
    "## [0, 1] indicates positive sentiment\n",
    "labels[np.arange(arr.size), arr] = 1\n",
    "print(f\"Label Shape: {labels.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check length of the encoded texts, which will allow us to pick a reasonable MAX_LEN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  1208\n"
     ]
    }
   ],
   "source": [
    "# Encode our concatenated data\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in df['text']]\n",
    "\n",
    "# Find the maximum length\n",
    "token_lens = [len(sent) for sent in encoded_tweets]\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hf/vxvn1cm55jncfc4dx4xp_q2h0000gn/T/ipykernel_30890/2976786349.py:4: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(token_lens)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 512]);\n",
    "plt.xlabel('Token count');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the sentences in the text column are too long. When these sentences are converted to tokens and sent inside the model, they exceed the 512 seq_length limit of the model. This is a problem as the embedding of the model used in the sentiment-analysis task was trained on 512 tokens embedding.\n",
    "\n",
    "To fix this issue we can either: \n",
    " \n",
    "1. Truncate the sentences with truncating = True\n",
    "\n",
    "2. Filter out the long sentences and keep only smaller ones\n",
    "\n",
    "```\n",
    "sentiment = classifier(data.iloc[i,0], truncation=True)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data inputs for the model\n",
    "\n",
    "Using the first method (truncating = True) to combat the issue of long sentences.\n",
    "\n",
    "Article: https://betterprogramming.pub/build-a-natural-language-classifier-with-bert-and-tensorflow-4770d4442d41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify max seq length of the model\n",
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5444, 512)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise two arrays for input tensors\n",
    "Xids = np.zeros((len(df), MAX_LEN))\n",
    "Xmask = np.zeros((len(df), MAX_LEN))\n",
    "Xids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 18:51:54.143877: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-31 18:52:02.137279: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# For each text in the dataframe...\n",
    "for i, sequence in enumerate(df['text']):\n",
    "    \n",
    "    # Return a dictionary containing the encoded sentence\n",
    "    tokens = tokenizer.encode_plus(str(sequence), max_length = MAX_LEN, \n",
    "                                   truncation = True,               # Needed since there are text seq > 512\n",
    "                                   padding = \"max_length\",          # For sentence < 512, padding is applied to reach a length of 512\n",
    "                                   add_special_tokens = True,       # Mark the start and end of sequences\n",
    "                                   return_token_type_ids = False, \n",
    "                                   return_attention_mask = True, \n",
    "                                   return_tensors = 'tf')           # Return TensorFlow object\n",
    "    \n",
    "    # Retrieve input_ids and attention_mask\n",
    "    ### input_ids : list of integers uniquely tied to a specific word\n",
    "    ### attention_mask : binary tokens indicating which tokens are the actual input tokens and which are padding tokens\n",
    "    Xids[i, :], Xmask[i, :] = tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101.,  7965.,  3899., ...,     0.,     0.,     0.],\n",
       "       [  101.,  7537.,  3019., ...,     0.,     0.,     0.],\n",
       "       [  101., 16957., 10768., ...,     0.,     0.,     0.],\n",
       "       ...,\n",
       "       [  101.,  4067.,  2643., ...,     0.,     0.,     0.],\n",
       "       [  101.,  4031.,  2204., ...,     0.,     0.,     0.],\n",
       "       [  101., 11498.,  7446., ...,     0.,     0.,     0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(512,), dtype=float64, numpy=\n",
      "array([  101.,  7965.,  3899.,  2833.,  2204., 17886.,  3258.,  2036.,\n",
      "        2204.,  2235., 17022.,  3899., 20323.,  5478.,  3815.,  2296.,\n",
      "        8521.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])>, <tf.Tensor: shape=(512,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)\n"
     ]
    }
   ],
   "source": [
    "# Combine arrays into tensorflow object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "\n",
    "# Display one training example\n",
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(512,), dtype=float64, numpy=\n",
      "array([  101.,  7965.,  3899.,  2833.,  2204., 17886.,  3258.,  2036.,\n",
      "        2204.,  2235., 17022.,  3899., 20323.,  5478.,  3815.,  2296.,\n",
      "        8521.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(512,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])>}, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)\n"
     ]
    }
   ],
   "source": [
    "# Create function to restructure the dataset\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks},labels\n",
    "\n",
    "# Apply map method to apply our function above to the dataset\n",
    "dataset = dataset.map(map_func)\n",
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(512,), dtype=float64, numpy=\n",
      "array([  101.,  2307.,  2126.,  4845.,  2131., 10962.,  2310., 13871.,\n",
      "        2666.,  5587., 10930., 27390.,  2102.,  5744.,  2100.,   102.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(512,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])>}, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data to prevent overfitting\n",
    "dataset = dataset.shuffle(100000, reshuffle_each_iteration = False)\n",
    "\n",
    "# Display one training example\n",
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5444"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain length of dataset\n",
    "DS_LEN = len(list(dataset))\n",
    "DS_LEN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train, test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify test-train split\n",
    "SPLIT = .8\n",
    "\n",
    "# Take or skip the specified number of batches to split by factor\n",
    "test = dataset.skip(round(DS_LEN * SPLIT)).batch(32)\n",
    "trainevalu = dataset.take(round(DS_LEN * SPLIT)) # 282\n",
    "\n",
    "DS_LEN2 = len(list(trainevalu))\n",
    "\n",
    "train = trainevalu.take(round(DS_LEN2 * SPLIT)).batch(32)\n",
    "evalu = trainevalu.skip(round(DS_LEN2 * SPLIT)).batch(32)\n",
    "\n",
    "# Uncomment the code below to delete dataset and free up disk space\n",
    "# del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: 35\n",
      "Train data: 109\n",
      "Train evaluation data: 28\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test data: {len(test)}\")\n",
    "print(f\"Train data: {len(train)}\")\n",
    "print(f\"Train evaluation data: {len(evalu)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "# Initialise BERT Model\n",
    "bertConfig = BertConfig.from_pretrained('bert-base-uncased', \n",
    "                                        output_hidden_states = True,\n",
    "                                        num_labels = 2,\n",
    "                                        max_length = MAX_LEN\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "tranformersPreTrainedModelName = 'bert-base-uncased'\n",
    "bert = TFBertForSequenceClassification.from_pretrained(tranformersPreTrainedModelName, config = bertConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=(                                               \n",
      "                                (None, 512, 768),                                                 \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768)),                                               \n",
      "                                 attentions=None, c                                               \n",
      "                                ross_attentions=Non                                               \n",
      "                                e)                                                                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 393216)       0           ['bert[0][13]']                  \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 393216)       0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 2)            786434      ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,268,674\n",
      "Trainable params: 110,268,674\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build 2 input layers to Bert Model where name needs to match the input values in the dataset\n",
    "input_ids = tf.keras.Input(shape = (MAX_LEN, ), name = 'input_ids', dtype = 'int32')\n",
    "mask = tf.keras.Input(shape = (MAX_LEN, ), name = 'attention_mask', dtype = 'int32')\n",
    "\n",
    "# Consume the last_hidden_state from BERT\n",
    "embedings = bert.layers[0](input_ids, attention_mask=mask)[0]\n",
    "\n",
    "# Original Author: Ferry Djaja\n",
    "# https://djajafer.medium.com/multi-class-text-classification-with-keras-and-lstm-4c5525bef592\n",
    "X = tf.keras.layers.Flatten()(embedings)\n",
    "X = tf.keras.layers.Dropout(0.5)(X)\n",
    "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids,mask], outputs=y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(train, validation_data = evalu, epochs = 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras.models.save_model(model, 'SA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = tf.keras.models.load_model(\"../../model/BERT_SA\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining best model metrics with callbacks\n",
    "\n",
    "Model Checkpoint: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
    "\n",
    "Early Stopping: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "\n",
    "Use learning rate scheduler to get decaying learning rate\n",
    "\n",
    "Learning Rate Scheduler: https://huggingface.co/course/chapter3/3?fw=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1090\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "num_train_steps = len(train)*EPOCHS\n",
    "print(num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "# Produce a decayed learning rate\n",
    "lr_scheduler = PolynomialDecay(initial_learning_rate = 5e-5, end_learning_rate = 2e-5, decay_steps = num_train_steps)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = lr_scheduler)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory and File path name to store checkpoints\n",
    "checkpoint_dir = 'tmp'\n",
    "checkpoint_filepath = 'tmp/checkpoint.hdf5'\n",
    "\n",
    "# Create file path to store the checkpoints\n",
    "## If a directory has already been created, comment out the code\n",
    "os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath,\n",
    "    save_weights_only = True,\n",
    "    monitor = 'val_accuracy',\n",
    "    mode = 'max',\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 3 # Stop training when there is no improvement in the loss after 3 consecutive epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history_callback \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train, validation_data \u001b[39m=\u001b[39;49m evalu, \n\u001b[1;32m      2\u001b[0m     epochs \u001b[39m=\u001b[39;49m EPOCHS, shuffle \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      3\u001b[0m     callbacks \u001b[39m=\u001b[39;49m [model_checkpoint_callback, early_stopping_callback])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:945\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    943\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    944\u001b[0m     \u001b[39m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    946\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    948\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_function_spec  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    949\u001b[0m       \u001b[39m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    950\u001b[0m           args, kwds))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:133\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m--> 133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m    134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mconcrete_function\u001b[39m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:360\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[1;32m    358\u001b[0m   args, kwargs \u001b[39m=\u001b[39m generalized_func_key\u001b[39m.\u001b[39m_placeholder_value()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(args, kwargs)\n\u001b[1;32m    362\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:284\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m    280\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m    281\u001b[0m ]\n\u001b[1;32m    282\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m    283\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 284\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    285\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m    286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m    287\u001b[0m         args,\n\u001b[1;32m    288\u001b[0m         kwargs,\n\u001b[1;32m    289\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    290\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m    291\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m    292\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m    293\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m    294\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m    295\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m    296\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    301\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1283\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1283\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1285\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:645\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    642\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    643\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    644\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 645\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    646\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1258\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[39m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[39;00m\n\u001b[1;32m   1257\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1258\u001b[0m   \u001b[39mreturn\u001b[39;00m autograph\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[1;32m   1259\u001b[0m       original_func,\n\u001b[1;32m   1260\u001b[0m       args,\n\u001b[1;32m   1261\u001b[0m       kwargs,\n\u001b[1;32m   1262\u001b[0m       options\u001b[39m=\u001b[39;49mautograph\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[1;32m   1263\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1264\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[1;32m   1265\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1266\u001b[0m       ))\n\u001b[1;32m   1267\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/var/folders/hf/vxvn1cm55jncfc4dx4xp_q2h0000gn/T/__autograph_generated_filee16dm14a.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(step_function), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m), ag__\u001b[39m.\u001b[39;49mld(iterator)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m conversion\u001b[39m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[1;32m    330\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: from cache\u001b[39m\u001b[39m'\u001b[39m, f)\n\u001b[0;32m--> 331\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mcontrol_status_ctx()\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m ag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mDISABLED:\n\u001b[1;32m    334\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: AutoGraph is disabled in context\u001b[39m\u001b[39m'\u001b[39m, f)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py:1233\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[1;32m   1230\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m     )\n\u001b[1;32m   1232\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1233\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[1;32m   1234\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1235\u001b[0m     outputs,\n\u001b[1;32m   1236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1237\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1238\u001b[0m )\n\u001b[1;32m   1239\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1316\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[1;32m   1312\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   1315\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1316\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2895\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2893\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m   2894\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[0;32m-> 2895\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3696\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3694\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   3695\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m-> 3696\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py:1222\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1222\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m   1223\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py:1027\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[1;32m   1026\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m-> 1027\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[1;32m   1028\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:526\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminimize\u001b[39m(\u001b[39mself\u001b[39m, loss, var_list, tape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    506\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \n\u001b[1;32m    508\u001b[0m \u001b[39m    This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[39m      None\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_gradients(loss, var_list, tape)\n\u001b[1;32m    527\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(grads_and_vars)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:259\u001b[0m, in \u001b[0;36m_BaseOptimizer.compute_gradients\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[39mif\u001b[39;00m callable(var_list):\n\u001b[1;32m    257\u001b[0m             var_list \u001b[39m=\u001b[39m var_list()\n\u001b[0;32m--> 259\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, var_list)\n\u001b[1;32m    260\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, var_list))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:1112\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1106\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1107\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1108\u001b[0m           output_gradients))\n\u001b[1;32m   1109\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1110\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1112\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1113\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1114\u001b[0m     flat_targets,\n\u001b[1;32m   1115\u001b[0m     flat_sources,\n\u001b[1;32m   1116\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1117\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1118\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1121\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:157\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    158\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:1369\u001b[0m, in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39mmul(grad, y), gen_math_ops\u001b[39m.\u001b[39mmul(grad, x)\n\u001b[1;32m   1366\u001b[0m \u001b[39massert\u001b[39;00m x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype \u001b[39m==\u001b[39m y\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, (x\u001b[39m.\u001b[39mdtype, \u001b[39m\"\u001b[39m\u001b[39m vs. \u001b[39m\u001b[39m\"\u001b[39m, y\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m   1368\u001b[0m (sx, rx, must_reduce_x), (sy, ry, must_reduce_y) \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1369\u001b[0m     SmartBroadcastGradientArgs(x, y, grad))\n\u001b[1;32m   1370\u001b[0m x \u001b[39m=\u001b[39m math_ops\u001b[39m.\u001b[39mconj(x)\n\u001b[1;32m   1371\u001b[0m y \u001b[39m=\u001b[39m math_ops\u001b[39m.\u001b[39mconj(y)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:104\u001b[0m, in \u001b[0;36mSmartBroadcastGradientArgs\u001b[0;34m(x, y, grad)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m (x_shape_tuple \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39min\u001b[39;00m x_shape_tuple \u001b[39mor\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     y_shape_tuple \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39min\u001b[39;00m y_shape_tuple):\n\u001b[1;32m    103\u001b[0m   sx \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mshape_internal(x, optimize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 104\u001b[0m   sy \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39;49mshape_internal(y, optimize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    105\u001b[0m   rx, ry \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39mbroadcast_gradient_args(sx, sy)\n\u001b[1;32m    106\u001b[0m   \u001b[39mreturn\u001b[39;00m (sx, rx, \u001b[39mTrue\u001b[39;00m), (sy, ry, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:697\u001b[0m, in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out_type:\n\u001b[1;32m    696\u001b[0m   out_type \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mint32\n\u001b[0;32m--> 697\u001b[0m \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mshape(\u001b[39minput\u001b[39;49m, name\u001b[39m=\u001b[39;49mname, out_type\u001b[39m=\u001b[39;49mout_type)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py:9372\u001b[0m, in \u001b[0;36mshape\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   9370\u001b[0m   out_type \u001b[39m=\u001b[39m _dtypes\u001b[39m.\u001b[39mint32\n\u001b[1;32m   9371\u001b[0m out_type \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39mmake_type(out_type, \u001b[39m\"\u001b[39m\u001b[39mout_type\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 9372\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[1;32m   9373\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mShape\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m, out_type\u001b[39m=\u001b[39;49mout_type, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   9374\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[1;32m   9375\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:754\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply_op_helper\u001b[39m(op_type_name, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkeywords):  \u001b[39m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Implementation of apply_op that returns output_structure, op.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 754\u001b[0m   op_def, g, producer \u001b[39m=\u001b[39m _GetOpDef(op_type_name, keywords)\n\u001b[1;32m    755\u001b[0m   name \u001b[39m=\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39melse\u001b[39;00m op_type_name\n\u001b[1;32m    757\u001b[0m   attrs, attr_protos \u001b[39m=\u001b[39m {}, {}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:734\u001b[0m, in \u001b[0;36m_GetOpDef\u001b[0;34m(op_type_name, keywords)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m   \u001b[39m# Need to flatten all the arguments into a list.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    733\u001b[0m   g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39m_get_graph_from_inputs(_Flatten(keywords\u001b[39m.\u001b[39mvalues()))\n\u001b[0;32m--> 734\u001b[0m   producer \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49mgraph_def_versions\u001b[39m.\u001b[39mproducer\n\u001b[1;32m    735\u001b[0m   \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:3422\u001b[0m, in \u001b[0;36mGraph.graph_def_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3420\u001b[0m   data \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_GetBuffer(buf)\n\u001b[1;32m   3421\u001b[0m version_def \u001b[39m=\u001b[39m versions_pb2\u001b[39m.\u001b[39mVersionDef()\n\u001b[0;32m-> 3422\u001b[0m version_def\u001b[39m.\u001b[39;49mParseFromString(compat\u001b[39m.\u001b[39;49mas_bytes(data))\n\u001b[1;32m   3423\u001b[0m \u001b[39mreturn\u001b[39;00m version_def\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_callback = model.fit(train, validation_data = evalu, \n",
    "    epochs = EPOCHS, shuffle = True, \n",
    "    callbacks = [model_checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model weights (that are considered the best) are loaded into the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "# Compile the model again to prepare for prediction\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the test data\n",
    "predicted = model.predict(test)\n",
    "y_predicted = np.argmax(predicted, axis = 1)\n",
    "\n",
    "# Retrieve true labels for test set\n",
    "test_true_labels = tf.concat([y for x, y in test], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of test data\n",
    "model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_predicted, test_true_labels)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['negative', 'positive'])\n",
    "cm_display.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data inputs for the model (2)\n",
    "\n",
    "Using the second method (filter) to combat the issue of long sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain indexes of encoded sentences that have length <= 150\n",
    "indexes = [i for i,v in enumerate(token_lens) if v <= 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant encoded sentences\n",
    "df_filter = df.iloc[indexes]\n",
    "\n",
    "# Display 5 random samples\n",
    "df_filter.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Shape: (5357, 2)\n",
      "Label Shape: (5357, 2)\n"
     ]
    }
   ],
   "source": [
    "# Extract labels values to get the size\n",
    "arr_filter = df_filter['labels'].values\n",
    "arr_filter.size\n",
    "\n",
    "# Creating 2D array to indicate which row of data the label belongs to\n",
    "labels_filter = np.zeros((arr_filter.size, arr_filter.max() + 1), dtype=int)\n",
    "print(f\"Label Shape: {labels_filter.shape}\")\n",
    "\n",
    "# Indicating the label (0 or 1) of the respective row of data\n",
    "labels_filter[np.arange(arr_filter.size), arr_filter] = 1\n",
    "print(f\"Label Shape: {labels_filter.shape}\")\n",
    "\n",
    "# Specify max seq length of the model\n",
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5357, 150)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise two arrays for input tensors\n",
    "Xids_filter = np.zeros((len(df_filter), MAX_LEN))\n",
    "Xmask_filter = np.zeros((len(df_filter), MAX_LEN))\n",
    "Xids_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# For each text in the dataframe...\n",
    "for i, sequence in enumerate(df_filter['text']):\n",
    "    \n",
    "    # Return a dictionary containing the encoded sentence\n",
    "    tokens = tokenizer.encode_plus(str(sequence), max_length = MAX_LEN, \n",
    "                                   truncation = False,              # Not needed since filtering was done\n",
    "                                   padding = \"max_length\",          # For sentence < 512, padding is applied to reach a length of 512\n",
    "                                   add_special_tokens = True,       # Mark the start and end of sequences\n",
    "                                   return_token_type_ids = False, \n",
    "                                   return_attention_mask = True, \n",
    "                                   return_tensors = 'tf')           # Return TensorFlow object\n",
    "    \n",
    "    # Retrieve input_ids and attention_mask\n",
    "    ### input_ids : list of integers uniquely tied to a specific word\n",
    "    ### attention_mask : binary tokens indicating which tokens are the actual input tokens and which are padding tokens\n",
    "    Xids_filter[i, :], Xmask_filter[i, :] = tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([  101.,  7965.,  3899.,  2833.,  2204., 17886.,  3258.,  2036.,\n",
      "        2204.,  2235., 17022.,  3899., 20323.,  5478.,  3815.,  2296.,\n",
      "        8521.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.])>, <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Combine arrays into tensorflow object\n",
    "dataset_filter = tf.data.Dataset.from_tensor_slices((Xids_filter, Xmask_filter, labels_filter))\n",
    "\n",
    "# Display one training example\n",
    "for i in dataset_filter.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([  101.,  7965.,  3899.,  2833.,  2204., 17886.,  3258.,  2036.,\n",
      "        2204.,  2235., 17022.,  3899., 20323.,  5478.,  3815.,  2296.,\n",
      "        8521.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>}, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Create function to restructure the dataset\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks},labels\n",
    "\n",
    "# Apply map method to apply our function above to the dataset\n",
    "dataset_filter = dataset_filter.map(map_func)\n",
    "\n",
    "for i in dataset_filter.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([  101.,  2367.,  2785., 16324.,  2028.,  2600.,  2052.,  2196.,\n",
      "        2113.,  1043.,  7630.,  6528.,  2489.,   102.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>}, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data\n",
    "dataset_filter = dataset_filter.shuffle(100000, reshuffle_each_iteration = False)\n",
    "\n",
    "# Display one training example\n",
    "for i in dataset_filter.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5357"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain length of dataset\n",
    "DS_LEN = len(list(dataset_filter))\n",
    "DS_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: 34\n",
      "Train data: 108\n",
      "Train evaluation data: 27\n"
     ]
    }
   ],
   "source": [
    "# Specify test-train split\n",
    "SPLIT = .8\n",
    "\n",
    "# Take or skip the specified number of batches to split by factor\n",
    "test_filter = dataset_filter.skip(round(DS_LEN * SPLIT)).batch(32)\n",
    "trainevalu_filter = dataset_filter.take(round(DS_LEN * SPLIT)) # 282\n",
    "\n",
    "DS_LEN2 = len(list(trainevalu_filter))\n",
    "\n",
    "train_filter = trainevalu_filter.take(round(DS_LEN2 * SPLIT)).batch(32)\n",
    "evalu_filter = trainevalu_filter.skip(round(DS_LEN2 * SPLIT)).batch(32)\n",
    "\n",
    "# Uncomment the code below to delete dataset and free up disk space\n",
    "# del dataset\n",
    "\n",
    "print(f\"Test data: {len(test_filter)}\")\n",
    "print(f\"Train data: {len(train_filter)}\")\n",
    "print(f\"Train evaluation data: {len(evalu_filter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 150,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=(                                               \n",
      "                                (None, 150, 768),                                                 \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768)),                                               \n",
      "                                 attentions=None, c                                               \n",
      "                                ross_attentions=Non                                               \n",
      "                                e)                                                                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 115200)       0           ['bert[0][13]']                  \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 115200)       0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 2)            230402      ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,712,642\n",
      "Trainable params: 109,712,642\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "tranformersPreTrainedModelName = 'bert-base-uncased'\n",
    "bert = TFBertForSequenceClassification.from_pretrained(tranformersPreTrainedModelName, config = bertConfig)\n",
    "\n",
    "# Build 2 input layers to Bert Model where name needs to match the input values in the dataset\n",
    "input_ids = tf.keras.Input(shape = (MAX_LEN, ), name = 'input_ids', dtype = 'int32')\n",
    "mask = tf.keras.Input(shape = (MAX_LEN, ), name = 'attention_mask', dtype = 'int32')\n",
    "\n",
    "# Consume the last_hidden_state from BERT\n",
    "embedings = bert.layers[0](input_ids, attention_mask=mask)[0]\n",
    "\n",
    "# Original Author: Ferry Djaja\n",
    "# https://djajafer.medium.com/multi-class-text-classification-with-keras-and-lstm-4c5525bef592\n",
    "X = tf.keras.layers.Flatten()(embedings)\n",
    "X = tf.keras.layers.Dropout(0.5)(X)\n",
    "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)\n",
    "\n",
    "model_filter = tf.keras.Model(inputs=[input_ids,mask], outputs=y)\n",
    "model_filter.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining best model metrics with callbacks (2)\n",
    "\n",
    "Model Checkpoint: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
    "\n",
    "Early Stopping: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "\n",
    "Use learning rate scheduler to get decaying learning rate\n",
    "\n",
    "Learning Rate Scheduler: https://huggingface.co/course/chapter3/3?fw=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "num_train_steps = len(train_filter)*EPOCHS\n",
    "print(num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "# Produce a decayed learning rate\n",
    "lr_scheduler = PolynomialDecay(initial_learning_rate = 5e-5, end_learning_rate = 2e-5, decay_steps = num_train_steps)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = lr_scheduler)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model_filter.compile(optimizer=opt, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to store metrics during checkpoints\n",
    "checkpoint_filepath_filter = 'tmp/filter_checkpoint.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filter_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath_filter,\n",
    "    save_weights_only = True,\n",
    "    monitor = 'val_accuracy',\n",
    "    mode = 'max',\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "early_stopping_filter_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 3 # Stop training when there is no improvement in the loss after 3 consecutive epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_filter_callback = model_filter.fit(train_filter, validation_data = evalu_filter, \n",
    "    epochs = EPOCHS, shuffle = True, \n",
    "    callbacks = [model_filter_checkpoint_callback, early_stopping_filter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model weights (that are considered the best) are loaded into the model\n",
    "model_filter.load_weights(checkpoint_filepath_filter)\n",
    "\n",
    "# Compile the model again to prepare for prediction\n",
    "model_filter.compile(optimizer=opt, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the test data\n",
    "predicted_filter = model_filter.predict(test_filter)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get classification report\n",
    "y_predicted_filter = np.argmax(predicted_filter, axis = 1)\n",
    "print(classification_report(test_filter['labels'], y_predicted_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_filter['labels'], predicted_filter)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['negative', 'positive'])\n",
    "cm_display.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Pipeline\n",
    "\n",
    "Function to convert raw data into input for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_pipeline(raw_data, sentence_column, labels_column, max_len = 512):\n",
    "    \n",
    "    import numpy as np\n",
    "    from transformers import BertTokenizer\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Extract out the necessary columns\n",
    "    print(\"Extracting out necessary columns...\")\n",
    "    df = raw_data[[sentence_column, labels_column]]\n",
    "\n",
    "    # Extract label values to get the size\n",
    "    arr = df[labels_column].values\n",
    "    print(f\"Total number of rows = {arr.size}\")\n",
    "\n",
    "    # Create 2D array to indicate which row of data the label belongs to\n",
    "    labels = np.zeros((arr.size, arr.max() + 1), dtype=int)\n",
    "\n",
    "    # Indicate the label (0 or 1) of the respective row of data\n",
    "    labels[np.arange(arr.size), arr] = 1\n",
    "    print(f\"Label Shape: {labels.shape}\")\n",
    "\n",
    "    # Load the BERT tokenizer\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    # Encode our concatenated data\n",
    "    print(\"Tokenizing sentences...\")\n",
    "    encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in df[sentence_column]]\n",
    "\n",
    "    # Initialise two arrays for input tensors\n",
    "    print(\"Initialising arrays for input tensors...\")\n",
    "    Xids = np.zeros((len(df), max_len))\n",
    "    Xmask = np.zeros((len(df), max_len))\n",
    "    print(f\"Input Tensors Shape: {Xids.shape}\")\n",
    "\n",
    "    print(\"Encoding sentences...\")\n",
    "    # For each text in the dataframe...\n",
    "    for i, sequence in enumerate(df[sentence_column]):\n",
    "        \n",
    "        # Return a dictionary containing the encoded sentence\n",
    "        tokens = tokenizer.encode_plus(str(sequence), max_length = max_len, \n",
    "                                    truncation = True,               # Needed since there are text seq > 512\n",
    "                                    padding = \"max_length\",          # For sentence < 512, padding is applied to reach a length of 512\n",
    "                                    add_special_tokens = True,       # Mark the start and end of sequences\n",
    "                                    return_token_type_ids = False, \n",
    "                                    return_attention_mask = True, \n",
    "                                    return_tensors = 'tf')           # Return TensorFlow object\n",
    "        \n",
    "        # Retrieve input_ids and attention_mask\n",
    "        ### input_ids : list of integers uniquely tied to a specific word\n",
    "        ### attention_mask : binary tokens indicating which tokens are the actual input tokens and which are padding tokens\n",
    "        Xids[i, :], Xmask[i, :] = tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "    # Combine arrays into tensorflow object\n",
    "    print(\"Creating Tensoflow Dataset...\")\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "\n",
    "    # Create function to restructure the dataset\n",
    "    def map_func(input_ids, masks, labels):\n",
    "        return {'input_ids': input_ids, 'attention_mask': masks},labels\n",
    "\n",
    "    # Apply map method to apply our function above to the dataset\n",
    "    dataset = dataset.map(map_func)\n",
    "\n",
    "    # Shuffle the data to prevent overfitting\n",
    "    print(\"Shuffling dataset...\")\n",
    "    dataset = dataset.shuffle(100000, reshuffle_each_iteration = False)\n",
    "\n",
    "    print(\"Success! Data is ready modelling!\")\n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
