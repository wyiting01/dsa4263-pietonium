{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>0</td>\n",
       "      <td>25/4/21</td>\n",
       "      <td>With no extra salt, it tastes a lot like fresh...</td>\n",
       "      <td>extra salt taste lot like fresh corn add salt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>0</td>\n",
       "      <td>1/3/21</td>\n",
       "      <td>The Maxwell House Peppermint Mocha is deliciou...</td>\n",
       "      <td>maxwell house peppermint mocha delicious easy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>0</td>\n",
       "      <td>18/1/21</td>\n",
       "      <td>We live in the middle of nowhere so we end up ...</td>\n",
       "      <td>live middle nowhere end order lot spice etc fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4424</th>\n",
       "      <td>1</td>\n",
       "      <td>7/9/21</td>\n",
       "      <td>I was surprised when the catnip arrived to fin...</td>\n",
       "      <td>surprised catnip arrive find one ounce tin lis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2554</th>\n",
       "      <td>0</td>\n",
       "      <td>25/5/21</td>\n",
       "      <td>These are really tasty. Wish I could read Chin...</td>\n",
       "      <td>really tasty wish could read chinese get idea ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment     Time                                               Text  \\\n",
       "3565          0  25/4/21  With no extra salt, it tastes a lot like fresh...   \n",
       "1922          0   1/3/21  The Maxwell House Peppermint Mocha is deliciou...   \n",
       "617           0  18/1/21  We live in the middle of nowhere so we end up ...   \n",
       "4424          1   7/9/21  I was surprised when the catnip arrived to fin...   \n",
       "2554          0  25/5/21  These are really tasty. Wish I could read Chin...   \n",
       "\n",
       "                                         processed_text  \n",
       "3565  extra salt taste lot like fresh corn add salt ...  \n",
       "1922  maxwell house peppermint mocha delicious easy ...  \n",
       "617   live middle nowhere end order lot spice etc fi...  \n",
       "4424  surprised catnip arrive find one ounce tin lis...  \n",
       "2554  really tasty wish could read chinese get idea ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read cleaned data\n",
    "df = pd.read_csv(\"../../data/curated/reviews/yiting_cleaned_reviews.csv\")\n",
    "\n",
    "# Display 5 random samples\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>healthy dog food good digestion also good smal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pleased natural balance dog food dog issue dog...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>educate feline nutrition allow cat become addi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>holistic vet recommend along brand try cat pre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buy coffee much cheaper ganocafe organic reish...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  healthy dog food good digestion also good smal...       0\n",
       "1  pleased natural balance dog food dog issue dog...       0\n",
       "2  educate feline nutrition allow cat become addi...       0\n",
       "3  holistic vet recommend along brand try cat pre...       0\n",
       "4  buy coffee much cheaper ganocafe organic reish...       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-labelling of columns headers\n",
    "df.rename(columns = {'Sentiment' : 'labels', 'processed_text' : 'text'}, inplace = True)\n",
    "\n",
    "# Extracting out the necessary columns\n",
    "df = df[['text','labels']]\n",
    "\n",
    "# Display the current dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5444"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels values to get the size\n",
    "arr = df['labels'].values\n",
    "arr.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Shape: (5444, 2)\n",
      "Label Shape: (5444, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating 2D array to indicate which row of data the label belongs to\n",
    "labels = np.zeros((arr.size, arr.max() + 1), dtype=int)\n",
    "print(f\"Label Shape: {labels.shape}\")\n",
    "\n",
    "# Indicating the label (0 or 1) of the respective row of data\n",
    "labels[np.arange(arr.size), arr] = 1\n",
    "print(f\"Label Shape: {labels.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check length of the encoded texts, which will allow us to pick a reasonable MAX_LEN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  1223\n"
     ]
    }
   ],
   "source": [
    "# Encode our concatenated data\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in df['text']]\n",
    "\n",
    "# Find the maximum length\n",
    "token_lens = [len(sent) for sent in encoded_tweets]\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/69njtlvd1pq8gqzx8tjvkdpc0000gp/T/ipykernel_37557/2976786349.py:4: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(token_lens)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 512]);\n",
    "plt.xlabel('Token count');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the sentences in the text column are too long. When these sentences are converted to tokens and sent inside the model, they exceed the 512 seq_length limit of the model. This is a problem as the embedding of the model used in the sentiment-analysis task was trained on 512 tokens embedding.\n",
    "\n",
    "To fix this issue we can either: \n",
    " \n",
    "1. Truncate the sentences with truncating = True\n",
    "\n",
    "2. Filter out the long sentences and keep only smaller ones\n",
    "\n",
    "```\n",
    "sentiment = classifier(data.iloc[i,0], truncation=True)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data inputs for the model\n",
    "\n",
    "Using the first method (truncating = True) to combat the issue of long sentences.\n",
    "\n",
    "Article: https://betterprogramming.pub/build-a-natural-language-classifier-with-bert-and-tensorflow-4770d4442d41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify max seq length of the model\n",
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5444, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise two arrays for input tensors\n",
    "Xids = np.zeros((len(df), MAX_LEN))\n",
    "Xmask = np.zeros((len(df), MAX_LEN))\n",
    "Xids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# For each text in the dataframe...\n",
    "for i, sequence in enumerate(df['text']):\n",
    "    \n",
    "    # Return a dictionary containing the encoded sentence\n",
    "    tokens = tokenizer.encode_plus(str(sequence), max_length = MAX_LEN, \n",
    "                                   truncation = True,               # Needed since there are text seq > 512\n",
    "                                   padding = \"max_length\",          # For sentence < 512, padding is applied to reach a length of 512\n",
    "                                   add_special_tokens = True,       # Mark the start and end of sequences\n",
    "                                   return_token_type_ids = False, \n",
    "                                   return_attention_mask = True, \n",
    "                                   return_tensors = 'tf')           # Return TensorFlow object\n",
    "    \n",
    "    # Retrieve input_ids and attention_mask\n",
    "    ### input_ids : list of integers uniquely tied to a specific word\n",
    "    ### attention_mask : binary tokens indicating which tokens are the actual input tokens and which are padding tokens\n",
    "    Xids[i, :], Xmask[i, :] = tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(512,), dtype=float64, numpy=\n",
      "array([  101.,  7965.,  3899.,  2833.,  2204., 17886.,  3258.,  2036.,\n",
      "        2204.,  2235., 17022.,  3899., 20323.,  5478.,  3815.,  2296.,\n",
      "        8521.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])>, <tf.Tensor: shape=(512,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Combine arrays into tensorflow object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "\n",
    "# Display one training example\n",
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(512,), dtype=float64, numpy=\n",
      "array([  101.,  7965.,  3899.,  2833.,  2204., 17886.,  3258.,  2036.,\n",
      "        2204.,  2235., 17022.,  3899., 20323.,  5478.,  3815.,  2296.,\n",
      "        8521.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(512,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])>}, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Create function to restructure the dataset\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks},labels\n",
    "\n",
    "# Apply map method to apply our function above to the dataset\n",
    "dataset = dataset.map(map_func)\n",
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(512,), dtype=float64, numpy=\n",
      "array([  101.,  2293.,  5510.,  2224.,  3769.,  9781.,  2191., 24593.,\n",
      "       12136.,  2100.,  2139.,  8566.,  6593.,  2732.,  2131.,  4658.,\n",
      "        7053.,  2143., 13675.,  2483.,  3597.,  2066.,  2460.,  7406.,\n",
      "        4412.,  2677.,  5223.,  9099.,  6638.,   102.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(512,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])>}, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data\n",
    "dataset = dataset.shuffle(100000, reshuffle_each_iteration = False)\n",
    "\n",
    "# Display one training example\n",
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5444"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain length of dataset\n",
    "DS_LEN = len(list(dataset))\n",
    "DS_LEN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train, test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify test-train split\n",
    "SPLIT = .8\n",
    "\n",
    "# Take or skip the specified number of batches to split by factor\n",
    "test = dataset.skip(round(DS_LEN * SPLIT)).batch(32)\n",
    "trainevalu = dataset.take(round(DS_LEN * SPLIT)) # 282\n",
    "\n",
    "DS_LEN2 = len(list(trainevalu))\n",
    "\n",
    "train = trainevalu.take(round(DS_LEN2 * SPLIT)).batch(32)\n",
    "evalu = trainevalu.skip(round(DS_LEN2 * SPLIT)).batch(32)\n",
    "\n",
    "# Uncomment the code below to delete dataset and free up disk space\n",
    "# del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: 35\n",
      "Train data: 109\n",
      "Train evaluation data: 28\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test data: {len(test)}\")\n",
    "print(f\"Train data: {len(train)}\")\n",
    "print(f\"Train evaluation data: {len(evalu)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "# Initialise BERT Model\n",
    "bertConfig = BertConfig.from_pretrained('bert-base-uncased', \n",
    "                                        output_hidden_states = True,\n",
    "                                        num_labels = 2,\n",
    "                                        max_length = MAX_LEN\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "tranformersPreTrainedModelName = 'bert-base-uncased'\n",
    "bert = TFBertForSequenceClassification.from_pretrained(tranformersPreTrainedModelName, config = bertConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=(                                               \n",
      "                                (None, 512, 768),                                                 \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768)),                                               \n",
      "                                 attentions=None, c                                               \n",
      "                                ross_attentions=Non                                               \n",
      "                                e)                                                                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 393216)       0           ['bert[0][13]']                  \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 393216)       0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 2)            786434      ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,268,674\n",
      "Trainable params: 110,268,674\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build 2 input layers to Bert Model where name needs to match the input values in the dataset\n",
    "input_ids = tf.keras.Input(shape = (MAX_LEN, ), name = 'input_ids', dtype = 'int32')\n",
    "mask = tf.keras.Input(shape = (MAX_LEN, ), name = 'attention_mask', dtype = 'int32')\n",
    "\n",
    "# Consume the last_hidden_state from BERT\n",
    "embedings = bert.layers[0](input_ids, attention_mask=mask)[0]\n",
    "\n",
    "# Original Author: Ferry Djaja\n",
    "# https://djajafer.medium.com/multi-class-text-classification-with-keras-and-lstm-4c5525bef592\n",
    "X = tf.keras.layers.Flatten()(embedings)\n",
    "X = tf.keras.layers.Dropout(0.5)(X)\n",
    "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids,mask], outputs=y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# history = model.fit(train, validation_data = evalu, epochs = 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras.models.save_model(model, 'SA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = tf.keras.models.load_model(\"../../model/BERT_SA\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining best model metrics with callbacks\n",
    "\n",
    "Model Checkpoint: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
    "\n",
    "Early Stopping: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "\n",
    "Use learning rate scheduler to get decaying learning rate\n",
    "\n",
    "Learning Rate Scheduler: https://huggingface.co/course/chapter3/3?fw=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "num_train_steps = len(train)*EPOCHS\n",
    "print(num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "# Produce a decayed learning rate\n",
    "lr_scheduler = PolynomialDecay(initial_learning_rate = 5e-5, end_learning_rate = 2e-5, decay_steps = num_train_steps)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = lr_scheduler)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path name to store checkpoints\n",
    "checkpoint_filepath = '/tmp/checkpoint'\n",
    "\n",
    "# Create file path to store the checkpoints\n",
    "## If a directory has already been created, comment out the code\n",
    "os.mkdir(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath,\n",
    "    save_weights_only = True,\n",
    "    monitor = 'val_accuracy',\n",
    "    mode = 'max',\n",
    "    save_best_only = True\n",
    ")\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 3 # Stop training when there is no improvement in the loss after 3 consecutive epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_callback = model.fit(train, validation_data = evalu, \n",
    "    epochs = EPOCHS, shuffle = True, \n",
    "    callbacks = [model_checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model weights (that are considered the best) are loaded into the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "# Compile the model again to prepare for prediction\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the test data\n",
    "predicted = model.predict(test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get classification report\n",
    "y_predicted = np.argmax(predicted, axis = 1)\n",
    "print(classification_report(test['labels'], y_predicted))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data inputs for the model (2)\n",
    "\n",
    "Using the second method (filter) to combat the issue of long sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain indexes of encoded sentences that have length <= 150\n",
    "indexes = [i for i,v in enumerate(token_lens) if v <= 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant encoded sentences\n",
    "df_filter = df.iloc[indexes]\n",
    "\n",
    "# Display 5 random samples\n",
    "df_filter.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Shape: (5357, 2)\n",
      "Label Shape: (5357, 2)\n"
     ]
    }
   ],
   "source": [
    "# Extract labels values to get the size\n",
    "arr_filter = df_filter['labels'].values\n",
    "arr_filter.size\n",
    "\n",
    "# Creating 2D array to indicate which row of data the label belongs to\n",
    "labels_filter = np.zeros((arr_filter.size, arr_filter.max() + 1), dtype=int)\n",
    "print(f\"Label Shape: {labels_filter.shape}\")\n",
    "\n",
    "# Indicating the label (0 or 1) of the respective row of data\n",
    "labels_filter[np.arange(arr_filter.size), arr_filter] = 1\n",
    "print(f\"Label Shape: {labels_filter.shape}\")\n",
    "\n",
    "# Specify max seq length of the model\n",
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5357, 150)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise two arrays for input tensors\n",
    "Xids_filter = np.zeros((len(df_filter), MAX_LEN))\n",
    "Xmask_filter = np.zeros((len(df_filter), MAX_LEN))\n",
    "Xids_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# For each text in the dataframe...\n",
    "for i, sequence in enumerate(df_filter['text']):\n",
    "    \n",
    "    # Return a dictionary containing the encoded sentence\n",
    "    tokens = tokenizer.encode_plus(str(sequence), max_length = MAX_LEN, \n",
    "                                   truncation = False,              # Not needed since filtering was done\n",
    "                                   padding = \"max_length\",          # For sentence < 512, padding is applied to reach a length of 512\n",
    "                                   add_special_tokens = True,       # Mark the start and end of sequences\n",
    "                                   return_token_type_ids = False, \n",
    "                                   return_attention_mask = True, \n",
    "                                   return_tensors = 'tf')           # Return TensorFlow object\n",
    "    \n",
    "    # Retrieve input_ids and attention_mask\n",
    "    ### input_ids : list of integers uniquely tied to a specific word\n",
    "    ### attention_mask : binary tokens indicating which tokens are the actual input tokens and which are padding tokens\n",
    "    Xids_filter[i, :], Xmask_filter[i, :] = tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([  101.,  7965.,  3899.,  2833.,  2204., 17886.,  3258.,  2036.,\n",
      "        2204.,  2235., 17022.,  3899., 20323.,  5478.,  3815.,  2296.,\n",
      "        8521.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.])>, <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Combine arrays into tensorflow object\n",
    "dataset_filter = tf.data.Dataset.from_tensor_slices((Xids_filter, Xmask_filter, labels_filter))\n",
    "\n",
    "# Display one training example\n",
    "for i in dataset_filter.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([  101.,  7965.,  3899.,  2833.,  2204., 17886.,  3258.,  2036.,\n",
      "        2204.,  2235., 17022.,  3899., 20323.,  5478.,  3815.,  2296.,\n",
      "        8521.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>}, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Create function to restructure the dataset\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks},labels\n",
    "\n",
    "# Apply map method to apply our function above to the dataset\n",
    "dataset_filter = dataset_filter.map(map_func)\n",
    "\n",
    "for i in dataset_filter.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([  101.,  2367.,  2785., 16324.,  2028.,  2600.,  2052.,  2196.,\n",
      "        2113.,  1043.,  7630.,  6528.,  2489.,   102.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>}, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data\n",
    "dataset_filter = dataset_filter.shuffle(100000, reshuffle_each_iteration = False)\n",
    "\n",
    "# Display one training example\n",
    "for i in dataset_filter.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5357"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain length of dataset\n",
    "DS_LEN = len(list(dataset_filter))\n",
    "DS_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: 34\n",
      "Train data: 108\n",
      "Train evaluation data: 27\n"
     ]
    }
   ],
   "source": [
    "# Specify test-train split\n",
    "SPLIT = .8\n",
    "\n",
    "# Take or skip the specified number of batches to split by factor\n",
    "test_filter = dataset_filter.skip(round(DS_LEN * SPLIT)).batch(32)\n",
    "trainevalu_filter = dataset_filter.take(round(DS_LEN * SPLIT)) # 282\n",
    "\n",
    "DS_LEN2 = len(list(trainevalu_filter))\n",
    "\n",
    "train_filter = trainevalu_filter.take(round(DS_LEN2 * SPLIT)).batch(32)\n",
    "evalu_filter = trainevalu_filter.skip(round(DS_LEN2 * SPLIT)).batch(32)\n",
    "\n",
    "# Uncomment the code below to delete dataset and free up disk space\n",
    "# del dataset\n",
    "\n",
    "print(f\"Test data: {len(test_filter)}\")\n",
    "print(f\"Train data: {len(train_filter)}\")\n",
    "print(f\"Train evaluation data: {len(evalu_filter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 150,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=(                                               \n",
      "                                (None, 150, 768),                                                 \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768)),                                               \n",
      "                                 attentions=None, c                                               \n",
      "                                ross_attentions=Non                                               \n",
      "                                e)                                                                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 115200)       0           ['bert[0][13]']                  \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 115200)       0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 2)            230402      ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,712,642\n",
      "Trainable params: 109,712,642\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "tranformersPreTrainedModelName = 'bert-base-uncased'\n",
    "bert = TFBertForSequenceClassification.from_pretrained(tranformersPreTrainedModelName, config = bertConfig)\n",
    "\n",
    "# Build 2 input layers to Bert Model where name needs to match the input values in the dataset\n",
    "input_ids = tf.keras.Input(shape = (MAX_LEN, ), name = 'input_ids', dtype = 'int32')\n",
    "mask = tf.keras.Input(shape = (MAX_LEN, ), name = 'attention_mask', dtype = 'int32')\n",
    "\n",
    "# Consume the last_hidden_state from BERT\n",
    "embedings = bert.layers[0](input_ids, attention_mask=mask)[0]\n",
    "\n",
    "# Original Author: Ferry Djaja\n",
    "# https://djajafer.medium.com/multi-class-text-classification-with-keras-and-lstm-4c5525bef592\n",
    "X = tf.keras.layers.Flatten()(embedings)\n",
    "X = tf.keras.layers.Dropout(0.5)(X)\n",
    "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)\n",
    "\n",
    "model_filter = tf.keras.Model(inputs=[input_ids,mask], outputs=y)\n",
    "model_filter.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining best model metrics with callbacks (2)\n",
    "\n",
    "Model Checkpoint: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
    "\n",
    "Early Stopping: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "\n",
    "Use learning rate scheduler to get decaying learning rate\n",
    "\n",
    "Learning Rate Scheduler: https://huggingface.co/course/chapter3/3?fw=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "num_train_steps = len(train_filter)*EPOCHS\n",
    "print(num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "# Produce a decayed learning rate\n",
    "lr_scheduler = PolynomialDecay(initial_learning_rate = 5e-5, end_learning_rate = 2e-5, decay_steps = num_train_steps)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = lr_scheduler)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model_filter.compile(optimizer=opt, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to store metrics during checkpoints\n",
    "checkpoint_filepath_filter = '/tmp/filter_checkpoint'\n",
    "\n",
    "# Create file path to store the checkpoints\n",
    "## If a directory has already been created, comment out the code\n",
    "os.mkdir(checkpoint_filepath_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filter_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath_filter,\n",
    "    save_weights_only = True,\n",
    "    monitor = 'val_accuracy',\n",
    "    mode = 'max',\n",
    "    save_best_only = True\n",
    ")\n",
    "\n",
    "early_stopping_filter_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 3 # Stop training when there is no improvement in the loss after 3 consecutive epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_filter_callback = model_filter.fit(train_filter, validation_data = evalu_filter, \n",
    "    epochs = EPOCHS, shuffle = True, \n",
    "    callbacks = [model_filter_checkpoint_callback, early_stopping_filter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model weights (that are considered the best) are loaded into the model\n",
    "model_filter.load_weights(checkpoint_filepath_filter)\n",
    "\n",
    "# Compile the model again to prepare for prediction\n",
    "model_filter.compile(optimizer=opt, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the test data\n",
    "predicted_filter = model_filter.predict(test_filter)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get classification report\n",
    "y_predicted_filter = np.argmax(predicted_filter, axis = 1)\n",
    "print(classification_report(test_filter['labels'], y_predicted_filter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
