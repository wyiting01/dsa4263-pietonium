{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/51/2c0959c5adf988c44d9e1e0d940f5b074516ecc87e96b1af25f59de9ba38/pip-23.0.1-py3-none-any.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 26.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "Successfully installed pip-23.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Collecting tensorflow==2.11.0\n",
      "  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.27.1\n",
      "  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /snap/jupyter/6/lib/python3.7/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (1.12.0)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in /snap/jupyter/6/lib/python3.7/site-packages (from tensorflow==2.11.0->-r requirements.txt (line 1)) (41.0.1)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.53.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m123.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-metadata\n",
      "  Downloading importlib_metadata-6.1.0-py3-none-any.whl (21 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.10.7-py3-none-any.whl (10 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.1/757.1 kB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /snap/jupyter/6/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 4)) (2.8.0)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /snap/jupyter/6/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow==2.11.0->-r requirements.txt (line 1)) (0.33.4)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.17.1-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.1/178.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.0/171.0 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /snap/jupyter/6/lib/python3.7/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (2019.3.9)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, tensorboard-plugin-wit, pytz, pyasn1, libclang, flatbuffers, zipp, wrapt, urllib3, typing-extensions, tqdm, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, regex, pyyaml, pyparsing, pyasn1-modules, protobuf, pillow, packaging, oauthlib, numpy, MarkupSafe, keras, joblib, idna, grpcio, google-pasta, gast, fonttools, filelock, cycler, charset-normalizer, cachetools, astunparse, absl-py, werkzeug, scipy, requests, pandas, opt-einsum, kiwisolver, importlib-metadata, h5py, google-auth, scikit-learn, requests-oauthlib, matplotlib, markdown, huggingface-hub, transformers, seaborn, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 charset-normalizer-3.1.0 cycler-0.11.0 filelock-3.10.7 flatbuffers-23.3.3 fonttools-4.38.0 gast-0.4.0 google-auth-2.17.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.53.0 h5py-3.8.0 huggingface-hub-0.13.3 idna-3.4 importlib-metadata-6.1.0 joblib-1.2.0 keras-2.11.0 kiwisolver-1.4.4 libclang-16.0.0 markdown-3.4.3 matplotlib-3.5.3 numpy-1.21.6 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.0 pandas-1.3.5 pillow-9.5.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.9 pytz-2023.3 pyyaml-6.0 regex-2022.10.31 requests-2.28.2 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-1.0.2 scipy-1.7.3 seaborn-0.12.2 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.2.0 threadpoolctl-3.1.0 tokenizers-0.13.2 tqdm-4.65.0 transformers-4.27.1 typing-extensions-4.5.0 urllib3-1.26.15 werkzeug-2.2.3 wrapt-1.15.0 zipp-3.15.0\n"
     ]
    }
   ],
   "source": [
    "# Install requirements\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5288</th>\n",
       "      <td>0</td>\n",
       "      <td>4/8/21</td>\n",
       "      <td>I purchased 2 other types of Blue Diamond almo...</td>\n",
       "      <td>purchase type blue diamond almond day around p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3490</th>\n",
       "      <td>1</td>\n",
       "      <td>24/1/21</td>\n",
       "      <td>A good product overall. It's just a base, so a...</td>\n",
       "      <td>good product overall base anything add step fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1</td>\n",
       "      <td>25/9/21</td>\n",
       "      <td>This is a good product for the cat's teeth. Yo...</td>\n",
       "      <td>good product cat teeth get cat interested give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>1</td>\n",
       "      <td>4/8/21</td>\n",
       "      <td>if u compare to buying the 3 packs at walmart,...</td>\n",
       "      <td>compare buy pack walmart great price pack walm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1</td>\n",
       "      <td>22/7/19</td>\n",
       "      <td>I love this organic coconut manna. I eat it by...</td>\n",
       "      <td>love organic coconut manna eat spoonful use co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment     Time                                               Text  \\\n",
       "5288          0   4/8/21  I purchased 2 other types of Blue Diamond almo...   \n",
       "3490          1  24/1/21  A good product overall. It's just a base, so a...   \n",
       "204           1  25/9/21  This is a good product for the cat's teeth. Yo...   \n",
       "2671          1   4/8/21  if u compare to buying the 3 packs at walmart,...   \n",
       "142           1  22/7/19  I love this organic coconut manna. I eat it by...   \n",
       "\n",
       "                                         processed_text  \n",
       "5288  purchase type blue diamond almond day around p...  \n",
       "3490  good product overall base anything add step fl...  \n",
       "204   good product cat teeth get cat interested give...  \n",
       "2671  compare buy pack walmart great price pack walm...  \n",
       "142   love organic coconut manna eat spoonful use co...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read cleaned data\n",
    "df = pd.read_csv(\"../../data/curated/reviews/cleaned_reviews.csv\")\n",
    "\n",
    "# Display 5 random samples\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>healthy dog food good digestion also good smal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pleased natural balance dog food dog issue dog...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>educate feline nutrition allow cat become addi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>holistic vet recommend along brand try cat pre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buy coffee much cheaper ganocafe organic reish...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  healthy dog food good digestion also good smal...       1\n",
       "1  pleased natural balance dog food dog issue dog...       1\n",
       "2  educate feline nutrition allow cat become addi...       1\n",
       "3  holistic vet recommend along brand try cat pre...       1\n",
       "4  buy coffee much cheaper ganocafe organic reish...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-labelling of columns headers\n",
    "df.rename(columns = {'Sentiment' : 'labels', 'processed_text' : 'text'}, inplace = True)\n",
    "\n",
    "# Extracting out the necessary columns\n",
    "df = df[['text','labels']]\n",
    "\n",
    "# Display the current dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5444"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels values to get the size\n",
    "arr = df['labels'].values\n",
    "arr.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Shape: (5444, 2)\n",
      "Label Shape: (5444, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating 2D array to indicate which row of data the label belongs to\n",
    "labels = np.zeros((arr.size, arr.max() + 1), dtype=int)\n",
    "print(f\"Label Shape: {labels.shape}\")\n",
    "\n",
    "# Indicating the label (0 or 1) of the respective row of data\n",
    "## [1, 0] indicates negative sentiment\n",
    "## [0, 1] indicates positive sentiment\n",
    "labels[np.arange(arr.size), arr] = 1\n",
    "print(f\"Label Shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 12:14:32.365104: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check length of the encoded texts, which will allow us to pick a reasonable MAX_LEN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  1208\n"
     ]
    }
   ],
   "source": [
    "# Encode our concatenated data\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in df['text']]\n",
    "\n",
    "# Find the maximum length\n",
    "token_lens = [len(sent) for sent in encoded_tweets]\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/jupyter/6/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 512]);\n",
    "plt.xlabel('Token count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the sentences in the text column are too long. When these sentences are converted to tokens and sent inside the model, they exceed the 512 seq_length limit of the model. This is a problem as the embedding of the model used in the sentiment-analysis task was trained on 512 tokens embedding.\n",
    "\n",
    "To fix this issue we can either: \n",
    " \n",
    "1. Truncate the sentences with truncating = True\n",
    "\n",
    "2. Filter out the long sentences and keep only smaller ones\n",
    "\n",
    "```\n",
    "sentiment = classifier(data.iloc[i,0], truncation=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data inputs for the model\n",
    "\n",
    "Using the first method (truncating = True) to combat the issue of long sentences.\n",
    "\n",
    "Article: https://betterprogramming.pub/build-a-natural-language-classifier-with-bert-and-tensorflow-4770d4442d41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify max seq length of the model\n",
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5444, 150)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise two arrays for input tensors\n",
    "Xids = np.zeros((len(df), MAX_LEN))\n",
    "Xmask = np.zeros((len(df), MAX_LEN))\n",
    "Xids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# For each text in the dataframe...\n",
    "for i, sequence in enumerate(df['text']):\n",
    "    \n",
    "    # Return a dictionary containing the encoded sentence\n",
    "    tokens = tokenizer.encode_plus(str(sequence), max_length = MAX_LEN, \n",
    "                                   truncation = True,               # Needed since there are text seq > 512\n",
    "                                   padding = \"max_length\",          # For sentence < 512, padding is applied to reach a length of 512\n",
    "                                   add_special_tokens = True,       # Mark the start and end of sequences\n",
    "                                   return_token_type_ids = False, \n",
    "                                   return_attention_mask = True, \n",
    "                                   return_tensors = 'tf')           # Return TensorFlow object\n",
    "    \n",
    "    # Retrieve input_ids and attention_mask\n",
    "    ### input_ids : list of integers uniquely tied to a specific word\n",
    "    ### attention_mask : binary tokens indicating which tokens are the actual input tokens and which are padding tokens\n",
    "    Xids[i, :], Xmask[i, :] = tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101.,  7965.,  3899., ...,     0.,     0.,     0.],\n",
       "       [  101.,  7537.,  3019., ...,     0.,     0.,     0.],\n",
       "       [  101., 16957., 10768., ...,     0.,     0.,     0.],\n",
       "       ...,\n",
       "       [  101.,  4067.,  2643., ...,     0.,     0.,     0.],\n",
       "       [  101.,  4031.,  2204., ...,     0.,     0.,     0.],\n",
       "       [  101., 11498.,  7446., ...,     0.,     0.,     0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([  101.,  7965.,  3899.,  2833.,  2204., 17886.,  3258.,  2036.,\n",
      "        2204.,  2235., 17022.,  3899., 20323.,  5478.,  3815.,  2296.,\n",
      "        8521.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.])>, <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)\n"
     ]
    }
   ],
   "source": [
    "# Combine arrays into tensorflow object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "\n",
    "# Display one training example\n",
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([  101.,  7965.,  3899.,  2833.,  2204., 17886.,  3258.,  2036.,\n",
      "        2204.,  2235., 17022.,  3899., 20323.,  5478.,  3815.,  2296.,\n",
      "        8521.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>}, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)\n"
     ]
    }
   ],
   "source": [
    "# Create function to restructure the dataset\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks},labels\n",
    "\n",
    "# Apply map method to apply our function above to the dataset\n",
    "dataset = dataset.map(map_func)\n",
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([  101.,  7078., 12090.,  2740.,  3771.,  5660.,  2100.,  2144.,\n",
      "        2191., 10500., 13724.,  5785., 23353., 11942., 10869.,  2738.,\n",
      "        2317., 13724.,  5699.,  4845.,  2293.,  5959., 19782.,  3221.,\n",
      "        6501.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>}, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data to prevent overfitting\n",
    "dataset = dataset.shuffle(100000, reshuffle_each_iteration = False)\n",
    "\n",
    "# Display one training example\n",
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5444"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain length of dataset\n",
    "DS_LEN = len(list(dataset))\n",
    "DS_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train, test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify test-train split\n",
    "SPLIT = .8\n",
    "\n",
    "# Take or skip the specified number of batches to split by factor\n",
    "test = dataset.skip(round(DS_LEN * SPLIT)).batch(32)\n",
    "trainevalu = dataset.take(round(DS_LEN * SPLIT)) # 282\n",
    "\n",
    "DS_LEN2 = len(list(trainevalu))\n",
    "\n",
    "train = trainevalu.take(round(DS_LEN2 * SPLIT)).batch(32)\n",
    "evalu = trainevalu.skip(round(DS_LEN2 * SPLIT)).batch(32)\n",
    "\n",
    "# Uncomment the code below to delete dataset and free up disk space\n",
    "# del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: 35\n",
      "Train data: 109\n",
      "Train evaluation data: 28\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test data: {len(test)}\")\n",
    "print(f\"Train data: {len(train)}\")\n",
    "print(f\"Train evaluation data: {len(evalu)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "# Initialise BERT Model\n",
    "bertConfig = BertConfig.from_pretrained('bert-base-uncased', \n",
    "                                        output_hidden_states = True,\n",
    "                                        num_labels = 2,\n",
    "                                        max_length = MAX_LEN\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "tranformersPreTrainedModelName = 'bert-base-uncased'\n",
    "bert = TFBertForSequenceClassification.from_pretrained(tranformersPreTrainedModelName, config = bertConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         multiple             109482240   ['input_ids[0][0]',              \n",
      "                                                                  'attention_mask[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 115200)       0           ['bert[1][13]']                  \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)           (None, 115200)       0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 2)            230402      ['dropout_39[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,712,642\n",
      "Trainable params: 109,712,642\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build 2 input layers to Bert Model where name needs to match the input values in the dataset\n",
    "input_ids = tf.keras.Input(shape = (MAX_LEN, ), name = 'input_ids', dtype = 'int32')\n",
    "mask = tf.keras.Input(shape = (MAX_LEN, ), name = 'attention_mask', dtype = 'int32')\n",
    "\n",
    "# Consume the last_hidden_state from BERT\n",
    "embedings = bert.layers[0](input_ids, attention_mask=mask)[0]\n",
    "\n",
    "# Original Author: Ferry Djaja\n",
    "# https://djajafer.medium.com/multi-class-text-classification-with-keras-and-lstm-4c5525bef592\n",
    "X = tf.keras.layers.Flatten()(embedings)\n",
    "X = tf.keras.layers.Dropout(0.5)(X)\n",
    "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids,mask], outputs=y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(train, validation_data = evalu, epochs = 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras.models.save_model(model, 'SA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = tf.keras.models.load_model(\"../../model/BERT_SA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining best model metrics with callbacks\n",
    "\n",
    "Model Checkpoint: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
    "\n",
    "Early Stopping: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "\n",
    "Use learning rate scheduler to get decaying learning rate\n",
    "\n",
    "Learning Rate Scheduler: https://huggingface.co/course/chapter3/3?fw=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1090\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "num_train_steps = len(train)*EPOCHS\n",
    "print(num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "# Produce a decayed learning rate\n",
    "lr_scheduler = PolynomialDecay(initial_learning_rate = 5e-5, end_learning_rate = 2e-5, decay_steps = num_train_steps)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = lr_scheduler)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory and File path name to store checkpoints\n",
    "checkpoint_dir = 'tmp'\n",
    "checkpoint_filepath = 'tmp/checkpoint.hdf5'\n",
    "\n",
    "# Create file path to store the checkpoints\n",
    "## If a directory has already been created, comment out the code\n",
    "#os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath,\n",
    "    save_weights_only = True,\n",
    "    monitor = 'val_accuracy',\n",
    "    mode = 'max',\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 3 # Stop training when there is no improvement in the loss after 3 consecutive epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.4890 - accuracy: 0.7821 - precision: 0.7821 - recall: 0.7821\n",
      "Epoch 1: val_accuracy improved from -inf to 0.84730, saving model to tmp/checkpoint.hdf5\n",
      "109/109 [==============================] - 644s 6s/step - loss: 0.4890 - accuracy: 0.7821 - precision: 0.7821 - recall: 0.7821 - val_loss: 0.3461 - val_accuracy: 0.8473 - val_precision: 0.8473 - val_recall: 0.8473\n",
      "Epoch 2/10\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9130 - precision: 0.9130 - recall: 0.9130\n",
      "Epoch 2: val_accuracy did not improve from 0.84730\n",
      "109/109 [==============================] - 605s 6s/step - loss: 0.2220 - accuracy: 0.9130 - precision: 0.9130 - recall: 0.9130 - val_loss: 0.4277 - val_accuracy: 0.8335 - val_precision: 0.8335 - val_recall: 0.8335\n",
      "Epoch 3/10\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9475 - precision: 0.9475 - recall: 0.9475\n",
      "Epoch 3: val_accuracy improved from 0.84730 to 0.88404, saving model to tmp/checkpoint.hdf5\n",
      "109/109 [==============================] - 606s 6s/step - loss: 0.1296 - accuracy: 0.9475 - precision: 0.9475 - recall: 0.9475 - val_loss: 0.3631 - val_accuracy: 0.8840 - val_precision: 0.8840 - val_recall: 0.8840\n",
      "Epoch 4/10\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9727 - precision: 0.9727 - recall: 0.9727\n",
      "Epoch 4: val_accuracy did not improve from 0.88404\n",
      "109/109 [==============================] - 603s 6s/step - loss: 0.0754 - accuracy: 0.9727 - precision: 0.9727 - recall: 0.9727 - val_loss: 0.5564 - val_accuracy: 0.8737 - val_precision: 0.8737 - val_recall: 0.8737\n"
     ]
    }
   ],
   "source": [
    "history_callback = model.fit(train, validation_data = evalu, \n",
    "    epochs = EPOCHS, shuffle = True, \n",
    "    callbacks = [model_checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model weights (that are considered the best) are loaded into the model\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "# Compile the model again to prepare for prediction\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of test data using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 55s 1s/step - loss: 0.2600 - accuracy: 0.8959 - precision_1: 0.9137 - recall_1: 0.9137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25998666882514954,\n",
       " 0.8959183692932129,\n",
       " 0.9136822819709778,\n",
       " 0.9136822819709778]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of test data\n",
    "model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 53s 2s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = tf.nn.softmax(model.predict(test))\n",
    "y_pred_argmax = tf.math.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_UnbatchDataset element_spec=({'input_ids': TensorSpec(shape=(150,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(150,), dtype=tf.float64, name=None)}, TensorSpec(shape=(2,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_un = test.unbatch()\n",
    "test_un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 1 0 ... 1 1 1], shape=(1089,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.Variable([], dtype=tf.int64)\n",
    "lst = []\n",
    "\n",
    "for features, label in test_un.take(-1):\n",
    "    lab = tf.math.argmax(label).numpy()\n",
    "    lst.append(lab)\n",
    "    \n",
    "arr = np.array(lst)\n",
    "tensor_object = tf.convert_to_tensor(arr)\n",
    "\n",
    "print(tensor_object)\n",
    "y_true = tensor_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.82       265\n",
      "           1       0.94      0.94      0.94       824\n",
      "\n",
      "    accuracy                           0.91      1089\n",
      "   macro avg       0.88      0.88      0.88      1089\n",
      "weighted avg       0.91      0.91      0.91      1089\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def visualize_confusion_matrix(y_pred_argmax, y_true):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_pred_arg: This is an array with values that are 0 or 1\n",
    "    :param y_true: This is an array with values that are 0 or 1\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    cm = tf.math.confusion_matrix(y_true, y_pred_argmax).numpy()\n",
    "    con_mat_df = pd.DataFrame(cm)\n",
    "    \n",
    "    print(classification_report(y_pred_argmax, y_true))\n",
    "\n",
    "    sns.heatmap(con_mat_df, annot=True, fmt='g', cmap=plt.cm.Blues)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "#print(classification_report(test_labels, baseline_predicted))\n",
    "visualize_confusion_matrix(y_pred_argmax, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data inputs for the model (2)\n",
    "\n",
    "Using the second method (filter) to combat the issue of long sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain indexes of encoded sentences that have length <= 150\n",
    "indexes = [i for i,v in enumerate(token_lens) if v <= 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant encoded sentences\n",
    "df_filter = df.iloc[indexes]\n",
    "\n",
    "# Display 5 random samples\n",
    "df_filter.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Shape: (5357, 2)\n",
      "Label Shape: (5357, 2)\n"
     ]
    }
   ],
   "source": [
    "# Extract labels values to get the size\n",
    "arr_filter = df_filter['labels'].values\n",
    "arr_filter.size\n",
    "\n",
    "# Creating 2D array to indicate which row of data the label belongs to\n",
    "labels_filter = np.zeros((arr_filter.size, arr_filter.max() + 1), dtype=int)\n",
    "print(f\"Label Shape: {labels_filter.shape}\")\n",
    "\n",
    "# Indicating the label (0 or 1) of the respective row of data\n",
    "labels_filter[np.arange(arr_filter.size), arr_filter] = 1\n",
    "print(f\"Label Shape: {labels_filter.shape}\")\n",
    "\n",
    "# Specify max seq length of the model\n",
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5357, 150)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise two arrays for input tensors\n",
    "Xids_filter = np.zeros((len(df_filter), MAX_LEN))\n",
    "Xmask_filter = np.zeros((len(df_filter), MAX_LEN))\n",
    "Xids_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# For each text in the dataframe...\n",
    "for i, sequence in enumerate(df_filter['text']):\n",
    "    \n",
    "    # Return a dictionary containing the encoded sentence\n",
    "    tokens = tokenizer.encode_plus(str(sequence), max_length = MAX_LEN, \n",
    "                                   truncation = False,              # Not needed since filtering was done\n",
    "                                   padding = \"max_length\",          # For sentence < 512, padding is applied to reach a length of 512\n",
    "                                   add_special_tokens = True,       # Mark the start and end of sequences\n",
    "                                   return_token_type_ids = False, \n",
    "                                   return_attention_mask = True, \n",
    "                                   return_tensors = 'tf')           # Return TensorFlow object\n",
    "    \n",
    "    # Retrieve input_ids and attention_mask\n",
    "    ### input_ids : list of integers uniquely tied to a specific word\n",
    "    ### attention_mask : binary tokens indicating which tokens are the actual input tokens and which are padding tokens\n",
    "    Xids_filter[i, :], Xmask_filter[i, :] = tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([  101.,  7965.,  3899.,  2833.,  2204., 17886.,  3258.,  2036.,\n",
      "        2204.,  2235., 17022.,  3899., 20323.,  5478.,  3815.,  2296.,\n",
      "        8521.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.])>, <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Combine arrays into tensorflow object\n",
    "dataset_filter = tf.data.Dataset.from_tensor_slices((Xids_filter, Xmask_filter, labels_filter))\n",
    "\n",
    "# Display one training example\n",
    "for i in dataset_filter.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([  101.,  7965.,  3899.,  2833.,  2204., 17886.,  3258.,  2036.,\n",
      "        2204.,  2235., 17022.,  3899., 20323.,  5478.,  3815.,  2296.,\n",
      "        8521.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>}, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Create function to restructure the dataset\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks},labels\n",
    "\n",
    "# Apply map method to apply our function above to the dataset\n",
    "dataset_filter = dataset_filter.map(map_func)\n",
    "\n",
    "for i in dataset_filter.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([  101.,  2367.,  2785., 16324.,  2028.,  2600.,  2052.,  2196.,\n",
      "        2113.,  1043.,  7630.,  6528.,  2489.,   102.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "           0.,     0.,     0.,     0.,     0.,     0.])>, 'attention_mask': <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>}, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data\n",
    "dataset_filter = dataset_filter.shuffle(100000, reshuffle_each_iteration = False)\n",
    "\n",
    "# Display one training example\n",
    "for i in dataset_filter.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5357"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain length of dataset\n",
    "DS_LEN = len(list(dataset_filter))\n",
    "DS_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: 34\n",
      "Train data: 108\n",
      "Train evaluation data: 27\n"
     ]
    }
   ],
   "source": [
    "# Specify test-train split\n",
    "SPLIT = .8\n",
    "\n",
    "# Take or skip the specified number of batches to split by factor\n",
    "test_filter = dataset_filter.skip(round(DS_LEN * SPLIT)).batch(32)\n",
    "trainevalu_filter = dataset_filter.take(round(DS_LEN * SPLIT)) # 282\n",
    "\n",
    "DS_LEN2 = len(list(trainevalu_filter))\n",
    "\n",
    "train_filter = trainevalu_filter.take(round(DS_LEN2 * SPLIT)).batch(32)\n",
    "evalu_filter = trainevalu_filter.skip(round(DS_LEN2 * SPLIT)).batch(32)\n",
    "\n",
    "# Uncomment the code below to delete dataset and free up disk space\n",
    "# del dataset\n",
    "\n",
    "print(f\"Test data: {len(test_filter)}\")\n",
    "print(f\"Train data: {len(train_filter)}\")\n",
    "print(f\"Train evaluation data: {len(evalu_filter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 150,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=(                                               \n",
      "                                (None, 150, 768),                                                 \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768),                                                \n",
      "                                 (None, 150, 768)),                                               \n",
      "                                 attentions=None, c                                               \n",
      "                                ross_attentions=Non                                               \n",
      "                                e)                                                                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 115200)       0           ['bert[0][13]']                  \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 115200)       0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 2)            230402      ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,712,642\n",
      "Trainable params: 109,712,642\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "tranformersPreTrainedModelName = 'bert-base-uncased'\n",
    "bert = TFBertForSequenceClassification.from_pretrained(tranformersPreTrainedModelName, config = bertConfig)\n",
    "\n",
    "# Build 2 input layers to Bert Model where name needs to match the input values in the dataset\n",
    "input_ids = tf.keras.Input(shape = (MAX_LEN, ), name = 'input_ids', dtype = 'int32')\n",
    "mask = tf.keras.Input(shape = (MAX_LEN, ), name = 'attention_mask', dtype = 'int32')\n",
    "\n",
    "# Consume the last_hidden_state from BERT\n",
    "embedings = bert.layers[0](input_ids, attention_mask=mask)[0]\n",
    "\n",
    "# Original Author: Ferry Djaja\n",
    "# https://djajafer.medium.com/multi-class-text-classification-with-keras-and-lstm-4c5525bef592\n",
    "X = tf.keras.layers.Flatten()(embedings)\n",
    "X = tf.keras.layers.Dropout(0.5)(X)\n",
    "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)\n",
    "\n",
    "model_filter = tf.keras.Model(inputs=[input_ids,mask], outputs=y)\n",
    "model_filter.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining best model metrics with callbacks (2)\n",
    "\n",
    "Model Checkpoint: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
    "\n",
    "Early Stopping: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "\n",
    "Use learning rate scheduler to get decaying learning rate\n",
    "\n",
    "Learning Rate Scheduler: https://huggingface.co/course/chapter3/3?fw=tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "num_train_steps = len(train_filter)*EPOCHS\n",
    "print(num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "# Produce a decayed learning rate\n",
    "lr_scheduler = PolynomialDecay(initial_learning_rate = 5e-5, end_learning_rate = 2e-5, decay_steps = num_train_steps)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = lr_scheduler)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model_filter.compile(optimizer=opt, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to store metrics during checkpoints\n",
    "checkpoint_filepath_filter = 'tmp/filter_checkpoint.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filter_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath_filter,\n",
    "    save_weights_only = True,\n",
    "    monitor = 'val_accuracy',\n",
    "    mode = 'max',\n",
    "    save_best_only = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "early_stopping_filter_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 3 # Stop training when there is no improvement in the loss after 3 consecutive epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_filter_callback = model_filter.fit(train_filter, validation_data = evalu_filter, \n",
    "    epochs = EPOCHS, shuffle = True, \n",
    "    callbacks = [model_filter_checkpoint_callback, early_stopping_filter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model weights (that are considered the best) are loaded into the model\n",
    "model_filter.load_weights(checkpoint_filepath_filter)\n",
    "\n",
    "# Compile the model again to prepare for prediction\n",
    "model_filter.compile(optimizer=opt, loss=loss, metrics=[acc, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the test data\n",
    "predicted_filter = model_filter.predict(test_filter)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get classification report\n",
    "y_predicted_filter = np.argmax(predicted_filter, axis = 1)\n",
    "print(classification_report(test_filter['labels'], y_predicted_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_filter['labels'], predicted_filter)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['negative', 'positive'])\n",
    "cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Pipeline\n",
    "\n",
    "Function to convert raw data into input for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_pipeline(raw_data, sentence_column, labels_column, max_len = 512):\n",
    "    \n",
    "    import numpy as np\n",
    "    from transformers import BertTokenizer\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Extract out the necessary columns\n",
    "    print(\"Extracting out necessary columns...\")\n",
    "    df = raw_data[[sentence_column, labels_column]]\n",
    "\n",
    "    # Extract label values to get the size\n",
    "    arr = df[labels_column].values\n",
    "    print(f\"Total number of rows = {arr.size}\")\n",
    "\n",
    "    # Create 2D array to indicate which row of data the label belongs to\n",
    "    labels = np.zeros((arr.size, arr.max() + 1), dtype=int)\n",
    "\n",
    "    # Indicate the label (0 or 1) of the respective row of data\n",
    "    labels[np.arange(arr.size), arr] = 1\n",
    "    print(f\"Label Shape: {labels.shape}\")\n",
    "\n",
    "    # Load the BERT tokenizer\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    # Encode our concatenated data\n",
    "    print(\"Tokenizing sentences...\")\n",
    "    encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in df[sentence_column]]\n",
    "\n",
    "    # Initialise two arrays for input tensors\n",
    "    print(\"Initialising arrays for input tensors...\")\n",
    "    Xids = np.zeros((len(df), max_len))\n",
    "    Xmask = np.zeros((len(df), max_len))\n",
    "    print(f\"Input Tensors Shape: {Xids.shape}\")\n",
    "\n",
    "    print(\"Encoding sentences...\")\n",
    "    # For each text in the dataframe...\n",
    "    for i, sequence in enumerate(df[sentence_column]):\n",
    "        \n",
    "        # Return a dictionary containing the encoded sentence\n",
    "        tokens = tokenizer.encode_plus(str(sequence), max_length = max_len, \n",
    "                                    truncation = True,               # Needed since there are text seq > 512\n",
    "                                    padding = \"max_length\",          # For sentence < 512, padding is applied to reach a length of 512\n",
    "                                    add_special_tokens = True,       # Mark the start and end of sequences\n",
    "                                    return_token_type_ids = False, \n",
    "                                    return_attention_mask = True, \n",
    "                                    return_tensors = 'tf')           # Return TensorFlow object\n",
    "        \n",
    "        # Retrieve input_ids and attention_mask\n",
    "        ### input_ids : list of integers uniquely tied to a specific word\n",
    "        ### attention_mask : binary tokens indicating which tokens are the actual input tokens and which are padding tokens\n",
    "        Xids[i, :], Xmask[i, :] = tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "    # Combine arrays into tensorflow object\n",
    "    print(\"Creating Tensoflow Dataset...\")\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "\n",
    "    # Create function to restructure the dataset\n",
    "    def map_func(input_ids, masks, labels):\n",
    "        return {'input_ids': input_ids, 'attention_mask': masks},labels\n",
    "\n",
    "    # Apply map method to apply our function above to the dataset\n",
    "    dataset = dataset.map(map_func)\n",
    "\n",
    "    # Shuffle the data to prevent overfitting\n",
    "    print(\"Shuffling dataset...\")\n",
    "    dataset = dataset.shuffle(100000, reshuffle_each_iteration = False)\n",
    "\n",
    "    print(\"Success! Data is ready modelling!\")\n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
