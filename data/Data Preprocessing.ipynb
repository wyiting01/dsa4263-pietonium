{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d368aae",
   "metadata": {},
   "source": [
    "This notebook contains functions to pre-process raw text inputs - Noise Entity Removal, Text Normalization and Conversion of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f862cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495cfeb",
   "metadata": {},
   "source": [
    "# 1. Noise Entity Removal\n",
    "This section is about noise entity removal. First, we convert all the words to lower case. We then remove html tags, non-word characters, digits and extra spaces. Finally, we remove stopwords from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a20bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_PATTERN = re.compile('<.*?>')\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS_LIST = set(stopwords.words('english'))\n",
    "\n",
    "def noise_entity_removal(target_input):\n",
    "    # convert to lower case\n",
    "    target_input = target_input.lower()\n",
    "    \n",
    "    # remove html tags\n",
    "    target_input = re.sub(HTML_PATTERN, ' ', target_input)\n",
    "    \n",
    "    # remove non-word characters like #,*,% etc\n",
    "    target_input = re.sub(r'\\W',' ', target_input)\n",
    "    \n",
    "    #will remove digits\n",
    "    target_input = re.sub(r'\\d',' ',target_input)\n",
    "    \n",
    "    #will remove extra spaces\n",
    "    target_input = re.sub(r'\\s+',' ',target_input)\n",
    "    \n",
    "    # remove stopwords\n",
    "    target_input_tokens = nltk.word_tokenize(target_input)\n",
    "    target_input_tokens_wo_stopwords = [i for i in target_input_tokens if i not in STOPWORDS_LIST and i]\n",
    "    \n",
    "    # join the list of tokens back to string\n",
    "    output = \" \".join(target_input_tokens_wo_stopwords)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1f3d0",
   "metadata": {},
   "source": [
    "## 2. Text Normalization\n",
    "In this section, we normalize our documents by either stemming or lemmatizing. Since lemmatization is able to retain the sentiment meanings, we will make lemmatization as the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21df895",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEMMATIZER = WordNetLemmatizer()\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "# POS Tags to be kept (Noun, Verb, Adjective, Adverb) (n,v,a,r)\n",
    "KEPT_POSTAGS = ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'VBZ', 'VBP', 'VBN', 'VBG','VBD', 'VB', 'RBS', 'RB', 'RBR']\n",
    "NOUN_POSTAGS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "VERB_POSTAGS = ['VBZ', 'VBP', 'VBN', 'VBG','VBD', 'VB']\n",
    "\n",
    "def mylemmatize(word, pos):\n",
    "    if pos in VERB_POSTAGS:\n",
    "        return LEMMATIZER.lemmatize(word, pos = 'v')\n",
    "    elif pos in NOUN_POSTAGS:\n",
    "        return LEMMATIZER.lemmatize(word, pos = 'n')\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def text_normalization(target_input, method = 'lemmatize'):\n",
    "    target_input_tokens = nltk.word_tokenize(target_input)\n",
    "    \n",
    "    if method == 'lemmatize':\n",
    "        #lemmatized_tokens = [LEMMATIZER.lemmatize(word, pos='v') for word in target_input_tokens]\n",
    "        lemmatized_tokens = [mylemmatize(*word_tup) for word_tup in nltk.pos_tag(target_input_tokens)]\n",
    "        output = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "    if method == 'stem':\n",
    "        stemmed_tokens = [STEMMER.stem(word) for word in target_input_tokens]\n",
    "        output = \" \".join(stemmed_tokens)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9e866",
   "metadata": {},
   "source": [
    "## 3. Stardardise Labels\n",
    "Since the dataset labels for sentiments are in the form of words (i.e. positive and negative), we will convert these labels to integers instead. <br>\n",
    "Positive: 0 <br>\n",
    "Negative: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720c39ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_integer(sentiment_label):\n",
    "    if sentiment_label == 'positive':\n",
    "        return 0\n",
    "    elif sentiment_label == 'negative':\n",
    "        return 1\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3da479",
   "metadata": {},
   "source": [
    "# Read in and Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9bd737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>18/6/21</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>7/7/21</td>\n",
       "      <td>I've been very pleased with the Natural Balanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>18/6/21</td>\n",
       "      <td>Before I was educated about feline nutrition, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>7/7/21</td>\n",
       "      <td>My holistic vet recommended this, along with a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>1/7/21</td>\n",
       "      <td>I bought this coffee because its much cheaper ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5439</th>\n",
       "      <td>negative</td>\n",
       "      <td>26/2/21</td>\n",
       "      <td>This is an okay gift box, only if you like med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>negative</td>\n",
       "      <td>18/12/19</td>\n",
       "      <td>It looks llike I just walked into a raw deal. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5441</th>\n",
       "      <td>negative</td>\n",
       "      <td>19/1/20</td>\n",
       "      <td>Thank god that i tasted the metal before i swa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5442</th>\n",
       "      <td>negative</td>\n",
       "      <td>13/9/20</td>\n",
       "      <td>This product was very good when I began buying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5443</th>\n",
       "      <td>negative</td>\n",
       "      <td>10/7/20</td>\n",
       "      <td>Once again, Paragon has disappointed with this...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5444 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiment      Time                                               Text\n",
       "0     positive   18/6/21  This is a very healthy dog food. Good for thei...\n",
       "1     positive    7/7/21  I've been very pleased with the Natural Balanc...\n",
       "2     positive   18/6/21  Before I was educated about feline nutrition, ...\n",
       "3     positive    7/7/21  My holistic vet recommended this, along with a...\n",
       "4     positive    1/7/21  I bought this coffee because its much cheaper ...\n",
       "...        ...       ...                                                ...\n",
       "5439  negative   26/2/21  This is an okay gift box, only if you like med...\n",
       "5440  negative  18/12/19  It looks llike I just walked into a raw deal. ...\n",
       "5441  negative   19/1/20  Thank god that i tasted the metal before i swa...\n",
       "5442  negative   13/9/20  This product was very good when I began buying...\n",
       "5443  negative   10/7/20  Once again, Paragon has disappointed with this...\n",
       "\n",
       "[5444 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('raw/reviews.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d69509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['Text'].apply(lambda x:noise_entity_removal(x))\n",
    "df['processed_text'] = df['processed_text'].apply(lambda x:text_normalization(x))\n",
    "df['Sentiment'] = df['Sentiment'].apply(lambda x:label_to_integer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae29514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('curated/reviews/yiting_cleaned_reviews.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
