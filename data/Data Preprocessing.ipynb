{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d368aae",
   "metadata": {},
   "source": [
    "This notebook contains functions to pre-process raw text inputs - Noise Entity Removal, Text Normalization and Conversion of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f862cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495cfeb",
   "metadata": {},
   "source": [
    "# 1. Noise Entity Removal\n",
    "This section is about noise entity removal. First, we convert all the words to lower case. We then remove html tags, non-word characters, digits and extra spaces. Finally, we remove stopwords from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a20bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_PATTERN = re.compile('<.*?>')\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS_LIST = set(stopwords.words('english'))\n",
    "\n",
    "def noise_entity_removal(target_input):\n",
    "    # convert to lower case\n",
    "    target_input = target_input.lower()\n",
    "    \n",
    "    # remove html tags\n",
    "    target_input = re.sub(HTML_PATTERN, ' ', target_input)\n",
    "    \n",
    "    # remove non-word characters like #,*,% etc\n",
    "    target_input = re.sub(r'\\W',' ', target_input)\n",
    "    \n",
    "    #will remove digits\n",
    "    target_input = re.sub(r'\\d',' ',target_input)\n",
    "    \n",
    "    #will remove extra spaces\n",
    "    target_input = re.sub(r'\\s+',' ',target_input)\n",
    "    \n",
    "    # remove stopwords\n",
    "    target_input_tokens = nltk.word_tokenize(target_input)\n",
    "    target_input_tokens_wo_stopwords = [i for i in target_input_tokens if i not in STOPWORDS_LIST and i]\n",
    "    \n",
    "    # join the list of tokens back to string\n",
    "    output = \" \".join(target_input_tokens_wo_stopwords)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1f3d0",
   "metadata": {},
   "source": [
    "## 2. Text Normalization\n",
    "In this section, we normalize our documents by either stemming or lemmatizing. Since lemmatization is able to retain the sentiment meanings, we will make lemmatization as the default. We are also removing tokens with length less than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21df895",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEMMATIZER = WordNetLemmatizer()\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "# POS Tags to be kept (Noun, Verb, Adjective, Adverb) (n,v,a,r)\n",
    "KEPT_POSTAGS = ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS', 'VBZ', 'VBP', 'VBN', 'VBG','VBD', 'VB', 'RBS', 'RB', 'RBR']\n",
    "NOUN_POSTAGS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "VERB_POSTAGS = ['VBZ', 'VBP', 'VBN', 'VBG','VBD', 'VB']\n",
    "\n",
    "def mylemmatize(word, pos):\n",
    "    if pos in VERB_POSTAGS:\n",
    "        return LEMMATIZER.lemmatize(word, pos = 'v')\n",
    "    elif pos in NOUN_POSTAGS:\n",
    "        return LEMMATIZER.lemmatize(word, pos = 'n')\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def text_normalization(target_input, method = 'lemmatize'):\n",
    "    target_input_tokens = nltk.word_tokenize(target_input)\n",
    "    \n",
    "    if method == 'lemmatize':\n",
    "        lemmatized_tokens = [mylemmatize(*word_tup) for word_tup in nltk.pos_tag(target_input_tokens)]\n",
    "        revised_lemmatized_tokens = [i for i in lemmatized_tokens if len(i) >= 3]\n",
    "        output = \" \".join(revised_lemmatized_tokens)\n",
    "    \n",
    "    if method == 'stem':\n",
    "        stemmed_tokens = [STEMMER.stem(word) for word in target_input_tokens]\n",
    "        revised_stemmed_tokens = [i for i in stemmed_tokens if len(i) >= 3]\n",
    "        output = \" \".join(revised_stemmed_tokens)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9e866",
   "metadata": {},
   "source": [
    "## 3. Stardardise Labels\n",
    "Since the dataset labels for sentiments are in the form of words (i.e. positive and negative), we will convert these labels to integers instead. <br>\n",
    "Positive: 0 <br>\n",
    "Negative: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c39ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_integer(sentiment_label):\n",
    "    if sentiment_label == 'positive':\n",
    "        return 0\n",
    "    elif sentiment_label == 'negative':\n",
    "        return 1\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3da479",
   "metadata": {},
   "source": [
    "# Read in and Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9bd737",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('raw/reviews.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d69509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['Text'].apply(lambda x:noise_entity_removal(x))\n",
    "df['processed_text'] = df['processed_text'].apply(lambda x:text_normalization(x))\n",
    "df['Sentiment'] = df['Sentiment'].apply(lambda x:label_to_integer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae29514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('curated/reviews/yiting_cleaned_reviews.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
